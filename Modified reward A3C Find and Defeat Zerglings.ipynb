{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C Agent for the minigame: Find and Defeat Zerglings\n",
    "\n",
    "The following notebook corresponds to the code used for training an A3C Agents for the SC2 minigame Find and Defeat Zerglings.\n",
    "\n",
    "The code has a modified reward system where the marines win a small reward if they go towards a Zerglings. With this small tweak we obtain better results in the short-term than the original system.\n",
    "\n",
    "This kind of modification is known as reward hacking, we are obtaining better results for our specific problem but we are moving away from a general solution.\n",
    "\n",
    "The original implementation was extracted from: https://github.com/greentfrapp/pysc2-RLagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import psutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.signal\n",
    "from time import sleep\n",
    "import os\n",
    "import sys\n",
    "from absl import flags\n",
    "from absl.flags import FLAGS\n",
    "\n",
    "# [NEW]\n",
    "from math import sqrt, isnan\n",
    "from skimage import measure\n",
    "########\n",
    "\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.env import environment\n",
    "from pysc2.lib import actions, features\n",
    "from pysc2.maps import mini_games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the notebooks in Windows (only plataform with the notebook tested). You have to modify the PySC2 init file as in:\n",
    "https://github.com/chris-chris/pysc2-examples/issues/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_PLAYER_FRIENDLY = 1\n",
    "_PLAYER_HOSTILE = 4\n",
    "INF = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command to see the TensorBoard\n",
    "#### Use the following command to launch Tensorboard:\n",
    "```\n",
    "tensorboard --logdir=worker_0:'./train_0',worker_1:'./train_1',worker_2:'./train_2',worker_3:'./train_3'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [NEW]\n",
    "def min_distance_to_enemy(obs, minimap=False):\n",
    "    obs = obs.observation\n",
    "    imin = obs['minimap'] if minimap else obs['screen']\n",
    "    imin = imin[_PLAYER_RELATIVE]\n",
    "    player_x, player_y = (imin == _PLAYER_FRIENDLY).nonzero()\n",
    "    enemy_x, enemy_y = (imin == _PLAYER_HOSTILE).nonzero()\n",
    "    min_sqdist = INF\n",
    "    for x, y in zip(enemy_x, enemy_y):\n",
    "        for x_, y_ in zip(player_x, player_y):\n",
    "            dx = x - x_\n",
    "            dy = y - y_\n",
    "            sqdist = dx*dx + dy*dy\n",
    "            if sqdist < min_sqdist: min_sqdist = sqdist\n",
    "    return sqrt(min_sqdist)\n",
    "\n",
    "def count_units(obs, minimap=False):\n",
    "    obs = obs.observation\n",
    "    imin = obs['minimap'] if minimap else obs['screen']\n",
    "    imin = imin[_PLAYER_RELATIVE]\n",
    "    _, number_of_units = measure.label(imin, connectivity=1, return_num=True)\n",
    "    return number_of_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processes PySC2 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_observation(observation, action_spec, observation_spec):\n",
    "    # reward\n",
    "    reward = observation.reward\n",
    "    # features\n",
    "    features = observation.observation\n",
    "    spatial_features = ['minimap', 'screen']\n",
    "    variable_features = ['cargo', 'multi_select', 'build_queue']\n",
    "    available_actions = ['available_actions']\n",
    "    # the shapes of some features depend on the state (eg. shape of multi_select depends on number of units)\n",
    "    # since tf requires fixed input shapes, we set a maximum size then pad the input if it falls short\n",
    "    max_no = {'available_actions': len(action_spec.functions), 'cargo': 500, 'multi_select': 500, 'build_queue': 10}\n",
    "    nonspatial_stack = []\n",
    "    for feature_label, feature in observation.observation.items():\n",
    "        if feature_label not in spatial_features + variable_features + available_actions:\n",
    "            nonspatial_stack = np.concatenate((nonspatial_stack, feature.reshape(-1)))\n",
    "        elif feature_label in variable_features:\n",
    "            padded_feature = np.concatenate((feature.reshape(-1), np.zeros(max_no[feature_label] * observation_spec['single_select'][1] - len(feature.reshape(-1)))))\n",
    "            nonspatial_stack = np.concatenate((nonspatial_stack, padded_feature))\n",
    "        elif feature_label in available_actions:\n",
    "            available_actions_feature = [1 if action_id in feature else 0 for action_id in np.arange(max_no['available_actions'])]\n",
    "            nonspatial_stack = np.concatenate((nonspatial_stack, available_actions_feature))\n",
    "    nonspatial_stack = np.expand_dims(nonspatial_stack, axis=0)\n",
    "    # spatial_minimap features\n",
    "    minimap_stack = np.expand_dims(np.stack(features['minimap'], axis=2), axis=0)\n",
    "    # spatial_screen features\n",
    "    screen_stack = np.expand_dims(np.stack(features['screen'], axis=2), axis=0)\n",
    "    # is episode over?\n",
    "    episode_end = observation.step_type == environment.StepType.LAST\n",
    "    return reward, nonspatial_stack, minimap_stack, screen_stack, episode_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discounting function used to calculate discounted returns.\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample from a given distribution\n",
    "def sample_dist(dist):\n",
    "    sample = np.random.choice(dist[0],p=dist[0])\n",
    "    sample = np.argmax(dist == sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTOR-CRITIC NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self, scope, trainer, action_spec, observation_spec):\n",
    "        with tf.variable_scope(scope):\n",
    "            # get size of features from action_spec and observation_spec\n",
    "            nonspatial_size = 0\n",
    "            spatial_features = ['minimap', 'screen']\n",
    "            initially_zero_features = {'cargo': 500, 'multi_select': 500, 'build_queue': 10, 'single_select': 1}\n",
    "            for feature_name, feature_dim in observation_spec.items():\n",
    "                if feature_name not in spatial_features:\n",
    "                    if feature_name == 'available_actions':\n",
    "                        feature_size = len(action_spec.functions)\n",
    "                    elif feature_name in initially_zero_features:\n",
    "                        feature_size = initially_zero_features[feature_name] * feature_dim[1]\n",
    "                    else:\n",
    "                        feature_size = 1\n",
    "                        for dim in feature_dim:\n",
    "                            feature_size *= dim\n",
    "                    nonspatial_size += feature_size\n",
    "            screen_channels = observation_spec['screen'][0]\n",
    "            minimap_channels = observation_spec['minimap'][0]\n",
    "\n",
    "            # Architecture here follows Atari-net Agent described in [1] Section 4.3\n",
    "\n",
    "            self.inputs_nonspatial = tf.placeholder(shape=[None,nonspatial_size], dtype=tf.float32)\n",
    "            self.inputs_spatial_screen = tf.placeholder(shape=[None,observation_spec['screen'][1],observation_spec['screen'][2],screen_channels], dtype=tf.float32)\n",
    "            self.inputs_spatial_minimap = tf.placeholder(shape=[None,observation_spec['minimap'][1],observation_spec['minimap'][2],minimap_channels], dtype=tf.float32)\n",
    "\n",
    "            self.nonspatial_dense = tf.layers.dense(\n",
    "                inputs=self.inputs_nonspatial,\n",
    "                units=32,\n",
    "                activation=tf.tanh)\n",
    "            self.screen_conv1 = tf.layers.conv2d(\n",
    "                inputs=self.inputs_spatial_screen,\n",
    "                filters=16,\n",
    "                kernel_size=[8,8],\n",
    "                strides=[4,4],\n",
    "                padding='valid',\n",
    "                activation=tf.nn.relu)\n",
    "            self.screen_conv2 = tf.layers.conv2d(\n",
    "                inputs=self.screen_conv1,\n",
    "                filters=32,\n",
    "                kernel_size=[4,4],\n",
    "                strides=[2,2],\n",
    "                padding='valid',\n",
    "                activation=tf.nn.relu)\n",
    "            self.minimap_conv1 = tf.layers.conv2d(\n",
    "                inputs=self.inputs_spatial_minimap,\n",
    "                filters=16,\n",
    "                kernel_size=[8,8],\n",
    "                strides=[4,4],\n",
    "                padding='valid',\n",
    "                activation=tf.nn.relu)\n",
    "            self.minimap_conv2 = tf.layers.conv2d(\n",
    "                inputs=self.minimap_conv1,\n",
    "                filters=32,\n",
    "                kernel_size=[4,4],\n",
    "                strides=[2,2],\n",
    "                padding='valid',\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            # According to [1]: \"The results are concatenated and sent through a linear layer with a ReLU activation.\"\n",
    "\n",
    "            screen_output_length = 1\n",
    "            for dim in self.screen_conv2.get_shape().as_list()[1:]:\n",
    "                screen_output_length *= dim\n",
    "            minimap_output_length = 1\n",
    "            for dim in self.minimap_conv2.get_shape().as_list()[1:]:\n",
    "                minimap_output_length *= dim\n",
    "\n",
    "            self.latent_vector = tf.layers.dense(\n",
    "                inputs=tf.concat([self.nonspatial_dense, tf.reshape(self.screen_conv2,shape=[-1,screen_output_length]), tf.reshape(self.minimap_conv2,shape=[-1,minimap_output_length])], axis=1),\n",
    "                units=256,\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            # Output layers for policy and value estimations\n",
    "            # 1 policy network for base actions\n",
    "            # 16 policy networks for arguments\n",
    "            #   - All modeled independently\n",
    "            #   - Spatial arguments have the x and y values modeled independently as well\n",
    "            # 1 value network\n",
    "            self.policy_base_actions = tf.layers.dense(\n",
    "                inputs=self.latent_vector,\n",
    "                units=len(action_spec.functions),\n",
    "                activation=tf.nn.softmax,\n",
    "                kernel_initializer=normalized_columns_initializer(0.01))\n",
    "            self.policy_arg = dict()\n",
    "            for arg in action_spec.types:\n",
    "                self.policy_arg[arg.name] = dict()\n",
    "                for dim, size in enumerate(arg.sizes):\n",
    "                    self.policy_arg[arg.name][dim] = tf.layers.dense(\n",
    "                        inputs=self.latent_vector,\n",
    "                        units=size,\n",
    "                        activation=tf.nn.softmax,\n",
    "                        kernel_initializer=normalized_columns_initializer(0.01))\n",
    "            self.value = tf.layers.dense(\n",
    "                inputs=self.latent_vector,\n",
    "                units=1,\n",
    "                kernel_initializer=normalized_columns_initializer(1.0))\n",
    "\n",
    "            # Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.actions_base = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "                self.actions_onehot_base = tf.one_hot(self.actions_base,524,dtype=tf.float32)\n",
    "\n",
    "                self.actions_arg = dict()\n",
    "                self.actions_onehot_arg = dict()\n",
    "                for arg in action_spec.types:\n",
    "                    self.actions_arg[arg.name] = dict()\n",
    "                    self.actions_onehot_arg[arg.name] = dict()\n",
    "                    for dim, size in enumerate(arg.sizes):\n",
    "                        self.actions_arg[arg.name][dim] = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "                        self.actions_onehot_arg[arg.name][dim] = tf.one_hot(self.actions_arg[arg.name][dim],size,dtype=tf.float32)\n",
    "\n",
    "                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "\n",
    "                self.responsible_outputs_base = tf.reduce_sum(self.policy_base_actions * self.actions_onehot_base, [1])\n",
    "\n",
    "                self.responsible_outputs_arg = dict()\n",
    "                for arg in action_spec.types:\n",
    "                    self.responsible_outputs_arg[arg.name] = dict()\n",
    "                    for dim, size in enumerate(arg.sizes):\n",
    "                        self.responsible_outputs_arg[arg.name][dim] = tf.reduce_sum(self.policy_arg[arg.name][dim] * self.actions_onehot_arg[arg.name][dim], [1])\n",
    "\n",
    "                # Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "\n",
    "                self.entropy_base = - tf.reduce_sum(self.policy_base_actions * tf.log(tf.clip_by_value(self.policy_base_actions, 1e-20, 1.0))) # avoid NaN with clipping when value in policy becomes zero\n",
    "\n",
    "                self.entropy_arg = dict()\n",
    "                for arg in action_spec.types:\n",
    "                    self.entropy_arg[arg.name] = dict()\n",
    "                    for dim, size in enumerate(arg.sizes):\n",
    "                        self.entropy_arg[arg.name][dim] = - tf.reduce_sum(self.policy_arg[arg.name][dim] * tf.log(tf.clip_by_value(self.policy_arg[arg.name][dim], 1e-20, 1.)))\n",
    "\n",
    "                self.entropy = self.entropy_base\n",
    "                for arg in action_spec.types:\n",
    "                    for dim, size in enumerate(arg.sizes):\n",
    "                        self.entropy += self.entropy_arg[arg.name][dim]\n",
    "\n",
    "                self.policy_loss_base = - tf.reduce_sum(tf.log(tf.clip_by_value(self.responsible_outputs_base, 1e-20, 1.0))*self.advantages)\n",
    "\n",
    "                self.policy_loss_arg = dict()\n",
    "                for arg in action_spec.types:\n",
    "                    self.policy_loss_arg[arg.name] = dict()\n",
    "                    for dim, size in enumerate(arg.sizes):\n",
    "                        self.policy_loss_arg[arg.name][dim] = - tf.reduce_sum(tf.log(tf.clip_by_value(self.responsible_outputs_arg[arg.name][dim], 1e-20, 1.0)) * self.advantages)\n",
    "\n",
    "                self.policy_loss = self.policy_loss_base\n",
    "                for arg in action_spec.types:\n",
    "                    for dim, size in enumerate(arg.sizes):\n",
    "                        self.policy_loss += self.policy_loss_arg[arg.name][dim]\n",
    "\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "                # Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                # self.gradients - gradients of loss wrt local_vars\n",
    "                self.gradients = tf.gradients(self.loss,local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)\n",
    "\n",
    "                # Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKER AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,name,trainer,model_path,global_episodes, map_name, action_spec, observation_spec):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name\n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        # [NEW]\n",
    "        self.episode_modified_rewards = []\n",
    "        #################\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.number))\n",
    "\n",
    "\n",
    "        # Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(self.name,trainer,action_spec,observation_spec)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)\n",
    "\n",
    "        print('Initializing environment #{}...'.format(self.number))\n",
    "        self.env = sc2_env.SC2Env(map_name=map_name)\n",
    "\n",
    "        self.action_spec = action_spec\n",
    "        self.observation_spec = observation_spec\n",
    "\n",
    "\n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        obs_screen = rollout[:,0]\n",
    "        obs_minimap = rollout[:,1]\n",
    "        obs_nonspatial = rollout[:,2]\n",
    "        actions_base = rollout[:,3]\n",
    "        actions_args = rollout[:,4]\n",
    "        rewards = rollout[:,5]\n",
    "        next_obs_screen = rollout[:,6]\n",
    "        next_obs_minimap = rollout[:,7]\n",
    "        next_obs_nonspatial = rollout[:,8]\n",
    "        values = rollout[:,10]\n",
    "\n",
    "        actions_arg_stack = dict()\n",
    "        for actions_arg in actions_args:\n",
    "            for arg_name,arg in actions_arg.items():\n",
    "                if arg_name not in actions_arg_stack:\n",
    "                    actions_arg_stack[arg_name] = dict()\n",
    "                for dim, value in arg.items():\n",
    "                    if dim not in actions_arg_stack[arg_name]:\n",
    "                        actions_arg_stack[arg_name][dim] = []\n",
    "                    actions_arg_stack[arg_name][dim].append(value)\n",
    "\n",
    "        # Here we take the rewards and values from the rollout, and use them to calculate the advantage and discounted returns\n",
    "        # The advantage function uses generalized advantage estimation from [2]\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.inputs_spatial_screen:np.stack(obs_screen).reshape(-1,64,64,17),\n",
    "            self.local_AC.inputs_spatial_minimap:np.stack(obs_minimap).reshape(-1,64,64,7),\n",
    "            self.local_AC.inputs_nonspatial:np.stack(obs_nonspatial).reshape(-1,7647),\n",
    "            self.local_AC.actions_base:actions_base,\n",
    "            self.local_AC.advantages:advantages}\n",
    "\n",
    "        for arg_name, arg in actions_arg_stack.items():\n",
    "            for dim, value in arg.items():\n",
    "                feed_dict[self.local_AC.actions_arg[arg_name][dim]] = value\n",
    "\n",
    "        v_l,p_l,e_l,g_n,v_n, _ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n,v_n\n",
    "\n",
    "    def work(self,max_episode_length,gamma,sess,coord,saver):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                                 \n",
    "            while not coord.should_stop():\n",
    "                # Download copy of parameters from global network\n",
    "                sess.run(self.update_local_ops)\n",
    "\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_modified_reward = 0\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                episode_end = False\n",
    "\n",
    "                # Start new episode\n",
    "                obs = self.env.reset()\n",
    "\n",
    "                # [NEW]\n",
    "                self.last_min_dist_to_enemy = min_distance_to_enemy(obs[0], minimap=True)\n",
    "                self.units_in_frame = count_units(obs[0], minimap=False)\n",
    "                #################\n",
    "\n",
    "                episode_frames.append(obs[0])\n",
    "                reward, nonspatial_stack, minimap_stack, screen_stack, episode_end = process_observation(obs[0], self.action_spec, self.observation_spec)\n",
    "                s_screen = screen_stack\n",
    "                s_minimap = minimap_stack\n",
    "                s_nonspatial = nonspatial_stack\n",
    "\n",
    "                while not episode_end:\n",
    "\n",
    "                    # Take an action using distributions from policy networks' outputs\n",
    "                    base_action_dist, arg_dist, v = sess.run([self.local_AC.policy_base_actions, self.local_AC.policy_arg, self.local_AC.value],\n",
    "                        feed_dict={self.local_AC.inputs_spatial_screen: screen_stack,\n",
    "                        self.local_AC.inputs_spatial_minimap: minimap_stack,\n",
    "                        self.local_AC.inputs_nonspatial: nonspatial_stack})\n",
    "\n",
    "                    # Apply filter to remove unavailable actions and then renormalize\n",
    "                    for action_id, action_prob in enumerate(base_action_dist[0]):\n",
    "                        if action_id not in obs[0].observation['available_actions']:\n",
    "                            base_action_dist[0][action_id] = 0\n",
    "                    if np.sum(base_action_dist[0]) != 1:\n",
    "                        current_sum = np.sum(base_action_dist[0])\n",
    "                        base_action_dist[0] /= current_sum\n",
    "\n",
    "                    base_action = sample_dist(base_action_dist)\n",
    "                    arg_sample = dict()\n",
    "                    for arg in arg_dist:\n",
    "                        arg_sample[arg] = dict()\n",
    "                        for dim in arg_dist[arg]:\n",
    "                            arg_sample[arg][dim] = sample_dist(arg_dist[arg][dim])\n",
    "\n",
    "                    arguments = []\n",
    "                    for arg in self.action_spec.functions[base_action].args:\n",
    "                        arg_value = []\n",
    "                        for dim, size in enumerate(arg.sizes):\n",
    "                            arg_value.append(arg_sample[arg.name][dim])\n",
    "                        arguments.append(arg_value)\n",
    "\n",
    "                    # Set unused arguments to -1 so that they won't be updated in the training\n",
    "                    # See documentation for tf.one_hot\n",
    "                    for arg_name, arg in arg_sample.items():\n",
    "                        if arg_name not in self.action_spec.functions[base_action].args:\n",
    "                            for dim in arg:\n",
    "                                arg_sample[arg_name][dim] = -1\n",
    "\n",
    "                    a = actions.FunctionCall(base_action, arguments)\n",
    "                    obs = self.env.step(actions=[a])\n",
    "\n",
    "                    r, nonspatial_stack, minimap_stack, screen_stack, episode_end = process_observation(obs[0], self.action_spec, self.observation_spec)\n",
    "\n",
    "                    # [NEW]\n",
    "                    r_modified = r\n",
    "\n",
    "                    last_dist = self.last_min_dist_to_enemy\n",
    "                    curr_dist = min_distance_to_enemy(obs[0], minimap=True)\n",
    "                    # if last_dist == INF and curr_dist < INF:\n",
    "                        # # print(\"Zergling discovered!\")\n",
    "                        # r_modified += 0.2 # Zergling discovered\n",
    "                    # elif last_dist < INF and curr_dist == INF:\n",
    "                        # if r <= 0 and not episode_end:\n",
    "                            # print(\"The marines have lost all the Zerglings!\")\n",
    "                            # r_modified -= 0.2 # # don't flee!\n",
    "                    # elif last_dist == INF and curr_dist == INF:\n",
    "                        # pass\n",
    "                        # # print(\"no zerglings\")\n",
    "                    if last_dist < INF and curr_dist < INF and r <= 0:\n",
    "                        r_modified += (last_dist - curr_dist)/20\n",
    "                        if isnan(r_modified): print(\"NaN at point A\")\n",
    "\n",
    "                    self.last_min_dist_to_enemy = curr_dist\n",
    "\n",
    "                    curr_units = count_units(obs[0], minimap=False)\n",
    "                    if base_action == 1:\n",
    "                        last_units = self.units_in_frame\n",
    "                        r_modified += 0.5*(curr_units - last_units)\n",
    "                        if isnan(r_modified): print(\"NaN at point B\")\n",
    "                        \"\"\"\n",
    "                        if curr_units > last_units:\n",
    "                            print(\"better camera frame\")\n",
    "                        elif curr_units < last_units:\n",
    "                            print(\"worse camera frame\")\n",
    "                        \"\"\"\n",
    "                    self.units_in_frame = curr_units\n",
    "                    #################\n",
    "\n",
    "                    if not episode_end:\n",
    "                        episode_frames.append(obs[0])\n",
    "                        s1_screen = screen_stack\n",
    "                        s1_minimap = minimap_stack\n",
    "                        s1_nonspatial = nonspatial_stack\n",
    "                    else:\n",
    "                        s1_screen = s_screen\n",
    "                        s1_minimap = s_minimap\n",
    "                        s1_nonspatial = s_nonspatial\n",
    "\n",
    "                    # Append latest state to buffer\n",
    "                    # [OLD]\n",
    "                    # episode_buffer.append([s_screen, s_minimap, s_nonspatial,base_action,arg_sample,r,s1_screen, s1_minimap, s1_nonspatial,episode_end,v[0,0]])\n",
    "                    # [NEW]\n",
    "                    episode_buffer.append([s_screen, s_minimap, s_nonspatial,base_action,arg_sample,r_modified,s1_screen, s1_minimap, s1_nonspatial,episode_end,v[0,0]])\n",
    "                    #################\n",
    "                    episode_values.append(v[0,0])\n",
    "\n",
    "                    # [NEW]\n",
    "                    episode_modified_reward += r_modified\n",
    "                    #################\n",
    "\n",
    "                    episode_reward += r\n",
    "                    s_screen = s1_screen\n",
    "                    s_minimap = s1_minimap\n",
    "                    s_nonspatial = s1_nonspatial\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "\n",
    "                    # If the episode hasn't ended, but the experience buffer is full, then we make an update step using that experience rollout\n",
    "                    if len(episode_buffer) == 30 and not episode_end and episode_step_count != max_episode_length - 1:\n",
    "                        # Since we don't know what the true final return is, we \"bootstrap\" from our current value estimation\n",
    "                        v1 = sess.run(self.local_AC.value,feed_dict={self.local_AC.inputs_spatial_screen: screen_stack,self.local_AC.inputs_spatial_minimap: minimap_stack,self.local_AC.inputs_nonspatial: nonspatial_stack})[0,0]\n",
    "                        v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,v1)\n",
    "                        episode_buffer = []\n",
    "                        sess.run(self.update_local_ops)\n",
    "                    if episode_end:\n",
    "                        break\n",
    "                # [NEW]\n",
    "                self.episode_modified_rewards.append(episode_modified_reward)\n",
    "                #################\n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                episode_count += 1\n",
    "\n",
    "                global _max_score, _running_avg_score, _episodes, _steps\n",
    "                if _max_score < episode_reward:\n",
    "                    _max_score = episode_reward\n",
    "                _running_avg_score = (2.0 / 101) * (episode_reward - _running_avg_score) + _running_avg_score\n",
    "                _episodes[self.number] = episode_count\n",
    "                _steps[self.number] = total_steps\n",
    "\n",
    "                # [NEW]\n",
    "                print(\"{} Step #{} Episode #{} Modified reward: {}\".format(self.name, total_steps, episode_count, episode_modified_reward))\n",
    "                # ###############\n",
    "\n",
    "                print(\"{} Step #{} Episode #{} Reward: {}\".format(self.name, total_steps, episode_count, episode_reward))\n",
    "                print(\"Total Steps: {}\\tTotal Episodes: {}\\tMax Score: {}\\tAvg Score: {}\".format(np.sum(_steps), np.sum(_episodes), _max_score, _running_avg_score))\n",
    "\n",
    "                # Update the network using the episode buffer at the end of the episode\n",
    "                if len(episode_buffer) != 0:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "\n",
    "                if episode_count % 5 == 0 and episode_count != 0:\n",
    "                    if episode_count % 250 == 0 and self.name == 'worker_0':\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print (\"Saved Model\")\n",
    "\n",
    "                    # [NEW]\n",
    "                    mean_modified_reward = np.mean(self.episode_modified_rewards[-5:])\n",
    "                    #################\n",
    "\n",
    "                    mean_reward = np.mean(self.episode_rewards[-5:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-5:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-5:])\n",
    "                    summary = tf.Summary()\n",
    "\n",
    "                    # [NEW]\n",
    "                    summary.value.add(tag='Perf/Modified Reward', simple_value=float(mean_modified_reward))\n",
    "                    #################\n",
    "\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                    summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                    summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                    summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                    summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing temporary environment to retrive action_spec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 23:28:48.396015  8188 sc_process.py:183] Killing the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing temporary environment to retrive observation_spec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 23:29:20.243906  8188 sc_process.py:183] Killing the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing environment #0...\n",
      "Initializing environment #1...\n",
      "Starting worker 0\n",
      "Starting worker 1\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worker_1 Step #67 Episode #1 Modified reward: -1.2359098897204284\n",
      "worker_1 Step #67 Episode #1 Reward: 1\n",
      "Total Steps: 67.0\tTotal Episodes: 1.0\tMax Score: 1\tAvg Score: 0.019801980198019802\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #159 Episode #2 Modified reward: 0.18141528991279499\n",
      "worker_1 Step #159 Episode #2 Reward: 3\n",
      "Total Steps: 159.0\tTotal Episodes: 2.0\tMax Score: 3\tAvg Score: 0.07881580237231645\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_1 Step #250 Episode #3 Modified reward: -1.8941840118674376\n",
      "worker_1 Step #250 Episode #3 Reward: 3\n",
      "Total Steps: 250.0\tTotal Episodes: 3.0\tMax Score: 3\tAvg Score: 0.1366610340085082\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_0 Step #359 Episode #1 Modified reward: 16.367742661057033\n",
      "worker_0 Step #359 Episode #1 Reward: 15\n",
      "Total Steps: 609.0\tTotal Episodes: 4.0\tMax Score: 15\tAvg Score: 0.4309845778895279\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #504 Episode #2 Modified reward: 0.5329049638742498\n",
      "worker_0 Step #504 Episode #2 Reward: 4\n",
      "Total Steps: 754.0\tTotal Episodes: 5.0\tMax Score: 15\tAvg Score: 0.5016581506045867\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_0 Step #561 Episode #3 Modified reward: 2.438516819594892\n",
      "worker_0 Step #561 Episode #3 Reward: 2\n",
      "Total Steps: 811.0\tTotal Episodes: 6.0\tMax Score: 15\tAvg Score: 0.5313282862361791\n",
      "worker_1 Step #609 Episode #4 Modified reward: -1.0674595841863712\n",
      "worker_1 Step #609 Episode #4 Reward: 2\n",
      "Total Steps: 1170.0\tTotal Episodes: 7.0\tMax Score: 15\tAvg Score: 0.5604108944295221\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #660 Episode #5 Modified reward: -1.9357838963105483\n",
      "worker_1 Step #660 Episode #5 Reward: -1\n",
      "Total Steps: 1221.0\tTotal Episodes: 8.0\tMax Score: 15\tAvg Score: 0.5295116687972543\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_0 Step #657 Episode #4 Modified reward: 3.2794297079869867\n",
      "worker_0 Step #657 Episode #4 Reward: 3\n",
      "Total Steps: 1317.0\tTotal Episodes: 9.0\tMax Score: 15\tAvg Score: 0.57843222981117\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #721 Episode #6 Modified reward: -1.5529018807067732\n",
      "worker_1 Step #721 Episode #6 Reward: 0\n",
      "Total Steps: 1378.0\tTotal Episodes: 10.0\tMax Score: 15\tAvg Score: 0.5669781262505528\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_0 Step #728 Episode #5 Modified reward: -4.0654621153992405\n",
      "worker_0 Step #728 Episode #5 Reward: 1\n",
      "Total Steps: 1449.0\tTotal Episodes: 11.0\tMax Score: 15\tAvg Score: 0.5755528168198487\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #812 Episode #7 Modified reward: -0.8602399491097203\n",
      "worker_1 Step #812 Episode #7 Reward: 1\n",
      "Total Steps: 1540.0\tTotal Episodes: 12.0\tMax Score: 15\tAvg Score: 0.5839577115362874\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_1 Step #905 Episode #8 Modified reward: -0.5016149411087261\n",
      "worker_1 Step #905 Episode #8 Reward: 1\n",
      "Total Steps: 1633.0\tTotal Episodes: 13.0\tMax Score: 15\tAvg Score: 0.5921961726939847\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #965 Episode #9 Modified reward: -4.458482046528079\n",
      "worker_1 Step #965 Episode #9 Reward: -1\n",
      "Total Steps: 1693.0\tTotal Episodes: 14.0\tMax Score: 15\tAvg Score: 0.5606675356109355\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #963 Episode #6 Modified reward: 3.4555569248867357\n",
      "worker_0 Step #963 Episode #6 Reward: 6\n",
      "Total Steps: 1928.0\tTotal Episodes: 15.0\tMax Score: 15\tAvg Score: 0.668377089361214\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #1008 Episode #10 Modified reward: 1.2999999999999998\n",
      "worker_1 Step #1008 Episode #10 Reward: -1\n",
      "Total Steps: 1971.0\tTotal Episodes: 16.0\tMax Score: 15\tAvg Score: 0.6353399192748533\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worker_0 Step #1052 Episode #7 Modified reward: 3.5724857994586054\n",
      "worker_0 Step #1052 Episode #7 Reward: 2\n",
      "Total Steps: 2060.0\tTotal Episodes: 17.0\tMax Score: 15\tAvg Score: 0.6623628911704008\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #1153 Episode #11 Modified reward: 6.421944084877413\n",
      "worker_1 Step #1153 Episode #11 Reward: 5\n",
      "Total Steps: 2205.0\tTotal Episodes: 18.0\tMax Score: 15\tAvg Score: 0.7482566953056404\n",
      "worker_0 Step #1154 Episode #8 Modified reward: 7.950488482339707\n",
      "worker_0 Step #1154 Episode #8 Reward: 6\n",
      "Total Steps: 2307.0\tTotal Episodes: 19.0\tMax Score: 15\tAvg Score: 0.8522516122302812\n",
      "worker_1 Step #1194 Episode #12 Modified reward: 0.6855150967112003\n",
      "worker_1 Step #1194 Episode #12 Reward: 0\n",
      "Total Steps: 2348.0\tTotal Episodes: 20.0\tMax Score: 15\tAvg Score: 0.8353753426811668\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #1211 Episode #9 Modified reward: -6.919766739519136\n",
      "worker_0 Step #1211 Episode #9 Reward: -1\n",
      "Total Steps: 2405.0\tTotal Episodes: 21.0\tMax Score: 15\tAvg Score: 0.7990312764894605\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_1 Step #1266 Episode #13 Modified reward: -1.9306593465682345\n",
      "worker_1 Step #1266 Episode #13 Reward: 0\n",
      "Total Steps: 2477.0\tTotal Episodes: 22.0\tMax Score: 15\tAvg Score: 0.7832088749748177\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #1312 Episode #14 Modified reward: -6.170710678118655\n",
      "worker_1 Step #1312 Episode #14 Reward: -2\n",
      "Total Steps: 2523.0\tTotal Episodes: 23.0\tMax Score: 15\tAvg Score: 0.7280958279456134\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_1 Step #1344 Episode #15 Modified reward: -5.689444872453601\n",
      "worker_1 Step #1344 Episode #15 Reward: -3\n",
      "Total Steps: 2555.0\tTotal Episodes: 24.0\tMax Score: 15\tAvg Score: 0.6542721481843141\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #1465 Episode #16 Modified reward: 5.299443683839306\n",
      "worker_1 Step #1465 Episode #16 Reward: 4\n",
      "Total Steps: 2676.0\tTotal Episodes: 25.0\tMax Score: 15\tAvg Score: 0.7205241848539317\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #1570 Episode #10 Modified reward: 3.218892154761478\n",
      "worker_0 Step #1570 Episode #10 Reward: 9\n",
      "Total Steps: 3035.0\tTotal Episodes: 26.0\tMax Score: 15\tAvg Score: 0.8844742009954379\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_0 Step #1625 Episode #11 Modified reward: 0.6398444494277431\n",
      "worker_0 Step #1625 Episode #11 Reward: -1\n",
      "Total Steps: 3090.0\tTotal Episodes: 27.0\tMax Score: 15\tAvg Score: 0.8471578801836471\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #1717 Episode #12 Modified reward: 1.1950236979882067\n",
      "worker_0 Step #1717 Episode #12 Reward: 1\n",
      "Total Steps: 3182.0\tTotal Episodes: 28.0\tMax Score: 15\tAvg Score: 0.8501844568136739\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #1824 Episode #17 Modified reward: 7.30848520427888\n",
      "worker_1 Step #1824 Episode #17 Reward: 9\n",
      "Total Steps: 3541.0\tTotal Episodes: 29.0\tMax Score: 15\tAvg Score: 1.0115669428173635\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #1849 Episode #13 Modified reward: 3.455276456678625\n",
      "worker_0 Step #1849 Episode #13 Reward: 5\n",
      "Total Steps: 3673.0\tTotal Episodes: 30.0\tMax Score: 15\tAvg Score: 1.0905458152368217\n",
      "worker_1 Step #1898 Episode #18 Modified reward: -3.4114769159601046\n",
      "worker_1 Step #1898 Episode #18 Reward: 0\n",
      "Total Steps: 3747.0\tTotal Episodes: 31.0\tMax Score: 15\tAvg Score: 1.068950848598469\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_1 Step #1990 Episode #19 Modified reward: 4.143323049723339\n",
      "worker_1 Step #1990 Episode #19 Reward: 3\n",
      "Total Steps: 3839.0\tTotal Episodes: 32.0\tMax Score: 15\tAvg Score: 1.107189445655925\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_0 Step #2029 Episode #14 Modified reward: 4.603102370682053\n",
      "worker_0 Step #2029 Episode #14 Reward: 7\n",
      "Total Steps: 4019.0\tTotal Episodes: 33.0\tMax Score: 15\tAvg Score: 1.2238787635637285\n",
      "worker_1 Step #2057 Episode #20 Modified reward: -1.4206832072829476\n",
      "worker_1 Step #2057 Episode #20 Reward: 1\n",
      "Total Steps: 4086.0\tTotal Episodes: 34.0\tMax Score: 15\tAvg Score: 1.2194455207208823\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #2066 Episode #15 Modified reward: -3.1594875162046674\n",
      "worker_0 Step #2066 Episode #15 Reward: -2\n",
      "Total Steps: 4123.0\tTotal Episodes: 35.0\tMax Score: 15\tAvg Score: 1.1556941242709637\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #2298 Episode #16 Modified reward: 6.138844208852969\n",
      "worker_0 Step #2298 Episode #16 Reward: 11\n",
      "Total Steps: 4355.0\tTotal Episodes: 36.0\tMax Score: 15\tAvg Score: 1.3506308742854\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #2345 Episode #17 Modified reward: -1.713932532413082\n",
      "worker_0 Step #2345 Episode #17 Reward: -1\n",
      "Total Steps: 4402.0\tTotal Episodes: 37.0\tMax Score: 15\tAvg Score: 1.3040837282599467\n",
      "worker_1 Step #2388 Episode #21 Modified reward: 5.351057956976286\n",
      "worker_1 Step #2388 Episode #21 Reward: 5\n",
      "Total Steps: 4733.0\tTotal Episodes: 38.0\tMax Score: 15\tAvg Score: 1.3772701890864825\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worker_0 Step #2407 Episode #18 Modified reward: -4.510248235922136\n",
      "worker_0 Step #2407 Episode #18 Reward: 1\n",
      "Total Steps: 4795.0\tTotal Episodes: 39.0\tMax Score: 15\tAvg Score: 1.3697994922728887\n",
      "better camera frame\n",
      "worker_1 Step #2483 Episode #22 Modified reward: 4.651718756691181\n",
      "worker_1 Step #2483 Episode #22 Reward: 2\n",
      "Total Steps: 4890.0\tTotal Episodes: 40.0\tMax Score: 15\tAvg Score: 1.382278710247683\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #2626 Episode #19 Modified reward: 8.150604582667576\n",
      "worker_0 Step #2626 Episode #19 Reward: 7\n",
      "Total Steps: 5109.0\tTotal Episodes: 41.0\tMax Score: 15\tAvg Score: 1.4935207159853527\n",
      "worker_1 Step #2673 Episode #23 Modified reward: 5.23657486801305\n",
      "worker_1 Step #2673 Episode #23 Reward: 5\n",
      "Total Steps: 5299.0\tTotal Episodes: 42.0\tMax Score: 15\tAvg Score: 1.5629559493321774\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_0 Step #2702 Episode #20 Modified reward: 1.6382553215083213\n",
      "worker_0 Step #2702 Episode #20 Reward: 1\n",
      "Total Steps: 5375.0\tTotal Episodes: 43.0\tMax Score: 15\tAvg Score: 1.5518083067711441\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worker_1 Step #2826 Episode #24 Modified reward: 1.6786658593424422\n",
      "worker_1 Step #2826 Episode #24 Reward: 3\n",
      "Total Steps: 5528.0\tTotal Episodes: 44.0\tMax Score: 15\tAvg Score: 1.5804853700033987\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #2839 Episode #21 Modified reward: 0.10180520720851427\n",
      "worker_0 Step #2839 Episode #21 Reward: 2\n",
      "Total Steps: 5665.0\tTotal Episodes: 45.0\tMax Score: 15\tAvg Score: 1.588792590399371\n",
      "worker_1 Step #2926 Episode #25 Modified reward: 5.592866981907697\n",
      "worker_1 Step #2926 Episode #25 Reward: 7\n",
      "Total Steps: 5765.0\tTotal Episodes: 46.0\tMax Score: 15\tAvg Score: 1.6959452123716605\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #3008 Episode #26 Modified reward: 3.398415493911952\n",
      "worker_1 Step #3008 Episode #26 Reward: 3\n",
      "Total Steps: 5847.0\tTotal Episodes: 47.0\tMax Score: 15\tAvg Score: 1.7217680794534098\n",
      "worse camera frame\n",
      "worker_0 Step #2991 Episode #22 Modified reward: 9.42950969173043\n",
      "worker_0 Step #2991 Episode #22 Reward: 7\n",
      "Total Steps: 5999.0\tTotal Episodes: 48.0\tMax Score: 15\tAvg Score: 1.8262875234246294\n",
      "worker_1 Step #3080 Episode #27 Modified reward: -0.25857315132410474\n",
      "worker_1 Step #3080 Episode #27 Reward: 0\n",
      "Total Steps: 6071.0\tTotal Episodes: 49.0\tMax Score: 15\tAvg Score: 1.7901234140498843\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_1 Step #3202 Episode #28 Modified reward: 5.068242975476189\n",
      "worker_1 Step #3202 Episode #28 Reward: 3\n",
      "Total Steps: 6193.0\tTotal Episodes: 50.0\tMax Score: 15\tAvg Score: 1.8140813662469162\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #3268 Episode #23 Modified reward: 10.050755661557892\n",
      "worker_0 Step #3268 Episode #23 Reward: 9\n",
      "Total Steps: 6470.0\tTotal Episodes: 51.0\tMax Score: 15\tAvg Score: 1.9563767847370763\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #3480 Episode #29 Modified reward: 9.250503399625105\n",
      "worker_1 Step #3480 Episode #29 Reward: 11\n",
      "Total Steps: 6748.0\tTotal Episodes: 52.0\tMax Score: 15\tAvg Score: 2.135458432564065\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_1 Step #3521 Episode #30 Modified reward: -2.6468871125850724\n",
      "worker_1 Step #3521 Episode #30 Reward: -3\n",
      "Total Steps: 6789.0\tTotal Episodes: 53.0\tMax Score: 15\tAvg Score: 2.0337661863746774\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #3521 Episode #24 Modified reward: 3.107581913152898\n",
      "worker_0 Step #3521 Episode #24 Reward: 6\n",
      "Total Steps: 7042.0\tTotal Episodes: 54.0\tMax Score: 15\tAvg Score: 2.1123054698128025\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #3638 Episode #31 Modified reward: 4.770972279952165\n",
      "worker_1 Step #3638 Episode #31 Reward: 5\n",
      "Total Steps: 7159.0\tTotal Episodes: 55.0\tMax Score: 15\tAvg Score: 2.1694875397174993\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #3788 Episode #25 Modified reward: 6.323186085015774\n",
      "worker_0 Step #3788 Episode #25 Reward: 11\n",
      "Total Steps: 7426.0\tTotal Episodes: 56.0\tMax Score: 15\tAvg Score: 2.3443491725943804\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #3841 Episode #26 Modified reward: 0.6388830452050605\n",
      "worker_0 Step #3841 Episode #26 Reward: 1\n",
      "Total Steps: 7479.0\tTotal Episodes: 57.0\tMax Score: 15\tAvg Score: 2.317728396899442\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #3997 Episode #32 Modified reward: 7.492047404966961\n",
      "worker_1 Step #3997 Episode #32 Reward: 14\n",
      "Total Steps: 7838.0\tTotal Episodes: 58.0\tMax Score: 15\tAvg Score: 2.5490605078519284\n",
      "worker_0 Step #3949 Episode #27 Modified reward: 2.266211199255433\n",
      "worker_0 Step #3949 Episode #27 Reward: 3\n",
      "Total Steps: 7946.0\tTotal Episodes: 59.0\tMax Score: 15\tAvg Score: 2.5579900027459495\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worker_0 Step #4060 Episode #28 Modified reward: 2.4532383855702142\n",
      "worker_0 Step #4060 Episode #28 Reward: 4\n",
      "Total Steps: 8057.0\tTotal Episodes: 60.0\tMax Score: 15\tAvg Score: 2.5865446561569208\n",
      "worker_1 Step #4141 Episode #33 Modified reward: -0.14960829097820372\n",
      "worker_1 Step #4141 Episode #33 Reward: 2\n",
      "Total Steps: 8201.0\tTotal Episodes: 61.0\tMax Score: 15\tAvg Score: 2.574929910490447\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #4205 Episode #29 Modified reward: 2.5236618527967636\n",
      "worker_0 Step #4205 Episode #29 Reward: 5\n",
      "Total Steps: 8346.0\tTotal Episodes: 62.0\tMax Score: 15\tAvg Score: 2.622951100381725\n",
      "worker_1 Step #4254 Episode #34 Modified reward: 0.6381146763045962\n",
      "worker_1 Step #4254 Episode #34 Reward: 5\n",
      "Total Steps: 8459.0\tTotal Episodes: 63.0\tMax Score: 15\tAvg Score: 2.670021375621691\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #4268 Episode #30 Modified reward: -2.0435779934564904\n",
      "worker_0 Step #4268 Episode #30 Reward: -1\n",
      "Total Steps: 8522.0\tTotal Episodes: 64.0\tMax Score: 15\tAvg Score: 2.5973476850153205\n",
      "worse camera frame\n",
      "worker_1 Step #4348 Episode #35 Modified reward: -0.5346542598089281\n",
      "worker_1 Step #4348 Episode #35 Reward: 3\n",
      "Total Steps: 8616.0\tTotal Episodes: 65.0\tMax Score: 15\tAvg Score: 2.605320998183334\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #4496 Episode #36 Modified reward: 9.234642987572318\n",
      "worker_1 Step #4496 Episode #36 Reward: 8\n",
      "Total Steps: 8764.0\tTotal Episodes: 66.0\tMax Score: 15\tAvg Score: 2.712146324951981\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "better camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #4627 Episode #31 Modified reward: 12.280024742355476\n",
      "worker_0 Step #4627 Episode #31 Reward: 12\n",
      "Total Steps: 9123.0\tTotal Episodes: 67.0\tMax Score: 15\tAvg Score: 2.8960642195073873\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_0 Step #4750 Episode #32 Modified reward: 2.07208335185506\n",
      "worker_0 Step #4750 Episode #32 Reward: 4\n",
      "Total Steps: 9246.0\tTotal Episodes: 68.0\tMax Score: 15\tAvg Score: 2.9179243339725875\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worse camera frame\n",
      "worker_1 Step #4855 Episode #37 Modified reward: 9.7860225202192\n",
      "worker_1 Step #4855 Episode #37 Reward: 10\n",
      "Total Steps: 9605.0\tTotal Episodes: 69.0\tMax Score: 15\tAvg Score: 3.0581634560721405\n",
      "worse camera frame\n",
      "worker_1 Step #4910 Episode #38 Modified reward: 0.3707106781186549\n",
      "worker_1 Step #4910 Episode #38 Reward: 0\n",
      "Total Steps: 9660.0\tTotal Episodes: 70.0\tMax Score: 15\tAvg Score: 2.9976057638726923\n",
      "worse camera frame\n",
      "worker_0 Step #4892 Episode #33 Modified reward: 2.742472235009762\n",
      "worker_0 Step #4892 Episode #33 Reward: 4\n",
      "Total Steps: 9802.0\tTotal Episodes: 71.0\tMax Score: 15\tAvg Score: 3.0174551546870942\n",
      "worse camera frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-11-b15ff6f12f70>\", line 49, in <lambda>\n",
      "    worker_work = lambda: worker.work(max_episode_length,gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-10-8170d3af58e8>\", line 283, in work\n",
      "    sess.run(self.increment)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\contextlib.py\", line 77, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3969, in get_controller\n",
      "    yield default\n",
      "  File \"<ipython-input-10-8170d3af58e8>\", line 283, in work\n",
      "    sess.run(self.increment)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\contextlib.py\", line 77, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3969, in get_controller\n",
      "    yield default\n",
      "  File \"<ipython-input-10-8170d3af58e8>\", line 224, in work\n",
      "    sess.run(self.update_local_ops)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1051, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-11-b15ff6f12f70>\", line 49, in <lambda>\n",
      "    worker_work = lambda: worker.work(max_episode_length,gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-10-8170d3af58e8>\", line 283, in work\n",
      "    sess.run(self.increment)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\contextlib.py\", line 77, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3969, in get_controller\n",
      "    yield default\n",
      "  File \"<ipython-input-10-8170d3af58e8>\", line 283, in work\n",
      "    sess.run(self.increment)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\contextlib.py\", line 77, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3969, in get_controller\n",
      "    yield default\n",
      "  File \"<ipython-input-10-8170d3af58e8>\", line 119, in work\n",
      "    self.local_AC.inputs_nonspatial: nonspatial_stack})\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\toolkits\\anaconda3-4.2.0\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1051, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b15ff6f12f70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mworker_threads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mcoord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworker_threads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\toolkits\\anaconda3-4.2.0\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[1;31m# Wait for all threads to stop or for request_stop() to be called.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[1;32mwhile\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\toolkits\\anaconda3-4.2.0\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\u001b[0m in \u001b[0;36mwait_for_stop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    309\u001b[0m       \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCoordinator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mtold\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mregister_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\toolkits\\anaconda3-4.2.0\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\toolkits\\anaconda3-4.2.0\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_episode_length = 300\n",
    "gamma = .99 # Discount rate for advantage estimation and reward discounting\n",
    "load_model = False\n",
    "model_path = './model'\n",
    "map_name = \"FindAndDefeatZerglings\"\n",
    "assert map_name in mini_games.mini_games\n",
    "\n",
    "print('Initializing temporary environment to retrive action_spec...')\n",
    "action_spec = sc2_env.SC2Env(map_name=map_name).action_spec()\n",
    "print('Initializing temporary environment to retrive observation_spec...')\n",
    "observation_spec = sc2_env.SC2Env(map_name=map_name).observation_spec()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    master_network = AC_Network('global',None, action_spec, observation_spec) # Generate global network\n",
    "    #num_workers = multiprocessing.cpu_count() # Set workers to number of available CPU threads\n",
    "    # Hardcoded to 2 workers to test with Windows\n",
    "    num_workers = 2 #psutil.cpu_count() # Set workers to number of available CPU threads\n",
    "    global _max_score, _running_avg_score, _steps, _episodes\n",
    "    _max_score = 0\n",
    "    _running_avg_score = 0\n",
    "    _steps = np.zeros(num_workers)\n",
    "    _episodes = np.zeros(num_workers)\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(i,trainer,model_path,global_episodes, map_name, action_spec, observation_spec))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # This is where the asynchronous magic happens\n",
    "    # Start the \"work\" process for each worker in a separate thread\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(max_episode_length,gamma,sess,coord,saver)\n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        t.start()\n",
    "        sleep(0.5)\n",
    "        worker_threads.append(t)\n",
    "    coord.join(worker_threads)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
